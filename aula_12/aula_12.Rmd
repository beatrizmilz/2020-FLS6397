---
word_document: default
author: "Beatriz Milz"
date: "26/06/2020"
output:
  html_document: null
 # word_document: default
code_folding: hide
title: "Aula 12 - Análise de texto"
---
Outro link de tutorial: http://www.leg.ufpr.br/~walmes/ensino/mintex/tutorials/03-sentimento.html

Dicas da Bruna W:
  - Pra fazer algo mais sofisticado você pode separar as palavras que vieram com NA e encontrar as palavras mais similares à elas que estão no dicionário de sentimentos.
  - lematização também ajuda
  - tirar os acentos ajuda também (abjutils::rm_accent())

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.align = "center")
```

```{r}
library(tidyverse)
library(nycflights13)
library(pdftools)
#install.packages("wordcloud")
#install.packages("tm")
library(wordcloud)
library(tm)
library(wordcloud2)
#install.packages("gutenbergr")
library(gutenbergr)
library(tidytext)
#install.packages("textstem")
library(textstem)
#install.packages("lexiconPT")
library(lexiconPT)
library(zoo)
#install.packages("topicmodels")
library(topicmodels)
library(broom)
```

## O que há em uma palavra? (str_length, str_detect)

```{r}
airports %>% slice(1) %>% pull(name)
```

```{r}
airports <- airports %>% mutate(caracteres=str_length(name))

airports
```

```{r}
airports2 <-
  airports %>% mutate(string_field = str_detect(name, "Field"))

airports2 %>% group_by(string_field) %>%
  tally()
```

```{r}
airports %>% mutate(string_regional = str_detect(name, "Regional|Rgnl")) %>%
  filter(string_regional == TRUE) %>%
  select(name)
```

```{r}
airports %>% mutate(string_z = str_detect(name, "^Z")) %>%
  filter(string_z == TRUE) %>%
  select(name)
```

```{r}
airports %>% mutate(string_ffgg = str_detect(name, "[fg]{2,}")) %>%
  filter(string_ffgg == TRUE) %>%
  select(name)
```

```{r}
artigo <- tibble(paginas = pdf_text("https://cutt.ly/Sy4vi7F"))

artigo
```

## Transformando Strings (str_replace, str_split)

```{r}
airports3 <- airports %>% mutate(name=str_replace(name, "Rgnl", "Regional")) %>% 
  mutate(name=str_replace(name, "-", " "))

```

```{r}
airports4 <- airports %>% mutate(nome_parcial=str_split(name, " "))
```

```{r}
airports5 <- airports4 %>% mutate(nome_parcial_primeiro=map_chr(nome_parcial, 1))
airports6 <- airports4 %>% mutate(nome_parcial_final=map_chr(nome_parcial, tail, n=1))

```

```{r}
airports7 <- airports %>% separate(tzone, "/", into=c("País", "Região"))
```

## Visualizando Strings

```{r}
airports %>% pull(name) %>% wordcloud::wordcloud()

```

## Tokenizer e Padronizar o Texto (unnest_tokens)

```{r}
Assis <- gutenberg_download(55752)
```


```{r}
Assis <- Assis %>% 
  mutate(text=iconv(text, from = "latin1", to = "UTF-8")) %>%
  select(-gutenberg_id) %>% 
  slice(21:8549) %>%
  rownames_to_column("Linha")
```

```{r}
Assis_palavras <- Assis %>% unnest_tokens(palavra, text, strip_numeric = TRUE)
```

```{r}
stopwords <- get_stopwords(language="pt") %>%
  rename(palavra=word) %>% add_row(palavra="é", lexicon="pessoal") %>% 
  add_row(palavra="à", lexicon="pessoal")

stopwords

Assis_palavras <- Assis_palavras %>% anti_join(stopwords, by="palavra")
```


```{r}
Assis_stem <- Assis_palavras %>% mutate(stem=stem_words(palavra, language="pt"))

```

```{r}
Assis_stem %>% distinct(palavra) %>% nrow()
Assis_stem %>% distinct(stem) %>% nrow()
```

```{r}
Assis_stem %>% sample_n(2000) %>% 
  pull(stem) %>% 
  wordcloud()
```

```{r}
Assis_stem %>% group_by(stem) %>%
  tally() %>%
  arrange(-n)
```

```{r}
Assis_stem %>% group_by(stem) %>%
  tally() %>%
  top_n(10, n) %>%
  mutate(stem=fct_reorder(stem, n)) %>%
  ggplot() +
  geom_col(aes(y=stem, x=n), fill="blue") +
  theme_minimal()
```

```{r}
Assis_stem %>% filter(stem=="govern")
```

## Análise de Sentimento
```{r}
sentimento <- oplexicon_v3.0 %>% select(term, polarity) %>%
  rename(palavra=term)

Assis_sentimentos <- Assis_stem %>% left_join(sentimento, by="palavra")
```

```{r}
Assis_sentimentos %>% summarize(sentimento = mean(polarity, na.rm = T))
```

```{r}
Assis_sentimentos %>% mutate(Linha=as.numeric(Linha)) %>%
  group_by(Linha) %>%
  summarize(polarity=sum(polarity, na.rm=T)) %>%
  arrange(-polarity) %>%
  slice(1, n())
```
```{r}
Assis %>% filter(Linha==7500) %>% pull(text) #Linha mais otimista


Assis %>% filter(Linha==1403) %>% pull(text) #Linha mais pessimista
```

```{r}
Assis_sentimentos %>% mutate(Linha=as.numeric(Linha)) %>%
  group_by(Linha) %>%
  summarize(polarity=sum(polarity, na.rm=T)) %>%
  complete(Linha=1:8527, fill=list(polarity=0)) %>%
  ggplot() +
  geom_line(aes(x=Linha, y=polarity)) +
  theme_classic() + 
  ylab("Sentimento")
```

```{r}


Assis_sentimentos %>% mutate(Linha = as.numeric(Linha)) %>%
  group_by(Linha) %>%
  summarize(polarity = sum(polarity, na.rm = T)) %>%
  complete(Linha = 1:8527, fill = list(polarity = 0)) %>%
  mutate(polarity_rolling = rollapply(polarity, 1000, mean, align = 'right', fill =
                                        NA)) %>%
  ggplot() +
  geom_line(aes(x = Linha, y = polarity_rolling)) +
  theme_classic() +
  ylab("Sentimento")
```

## Ngrams

```{r}
Assis_bigrams <- Assis %>% unnest_tokens(bigram, text,
                                       token="ngrams", n=2)
```

```{r}
Assis_bigrams <- Assis_bigrams %>% 
  separate(bigram, c("palavra1", "palavra2"), sep=" ") %>% 
  anti_join(stopwords, by=c("palavra1"="palavra")) %>%
  anti_join(stopwords, by=c("palavra2"="palavra")) %>%
  unite("bigram", c(palavra1, palavra2), sep=" ", remove=F)
```

```{r}
Assis_bigrams %>%
  group_by(bigram) %>%
  tally() %>%
  arrange(-n)
```
```{r}
Assis_bigrams %>% filter(palavra1=="familia") %>% pull(palavra2)

Assis_bigrams %>% filter(palavra2=="familia") %>% pull(palavra1)
```


```{r}
Assis_trigrams <- Assis %>% unnest_tokens(trigram, text,
                                          token = "ngrams", n = 3)

Assis_trigrams <- Assis_trigrams %>%
  separate(trigram, c("palavra1", "palavra2", "palavra3"), sep = " ") %>%
  anti_join(stopwords, by = c("palavra1" = "palavra")) %>%
  anti_join(stopwords, by = c("palavra2" = "palavra")) %>%
  anti_join(stopwords, by = c("palavra3" = "palavra")) %>%
  unite("trigram",
        c(palavra1, palavra2, palavra3),
        sep = " ",
        remove = F)
Assis_trigrams %>%
  group_by(trigram) %>%
  tally() %>%
  arrange(-n)
```

## Comparando Documentos

```{r}

Verne <- gutenberg_download(28341) %>%
  mutate(text = iconv(text, from = "latin1", to = "UTF-8")) %>%
  select(-gutenberg_id) %>%
  slice(71:7229) %>%
  rownames_to_column("Linha")
```

```{r}
Verne_palavras <- Verne %>% unnest_tokens(palavra, text, strip_numeric=TRUE) %>%
  anti_join(stopwords, by="palavra") %>%
  mutate(stem=stem_words(palavra, language="pt"))
```

```{r}
Assis_prep <- Assis_palavras %>% 
  group_by(palavra) %>% 
  tally() %>%
  mutate(document="Assis")

Verne_prep <- Verne_palavras %>% 
  group_by(palavra) %>% 
  tally() %>%
  mutate(document="Verne")


Assis_Verne <- Assis_prep %>% bind_rows(Verne_prep)
```

## Frequência Relativa de Palavras (bind_tf_idf)

```{r}
Assis_Verne_idf <- Assis_Verne %>% bind_tf_idf(palavra, document, n)
```

```{r}
Assis_Verne_idf %>%
  group_by(document) %>%
  top_n(5, tf_idf)
```


## Modelagem de Tópicos

```{r}
Assis_Verne_dtm <- Assis_Verne %>%
  cast_dtm(document, palavra, n)
```

```{r}
Assis_Verne_LDA <- LDA(Assis_Verne_dtm, 4)
```

```{r}
Assis_Verne_LDA_documentos <- Assis_Verne_LDA %>% tidy(matrix='gamma')
Assis_Verne_LDA_documentos
```


```{r}
Assis_Verne_LDA_palavras <- Assis_Verne_LDA %>% tidy(matrix='beta')
```

```{r}
Assis_Verne_LDA_palavras %>% group_by(topic) %>%
  top_n(5, beta) %>%
  arrange(topic, -beta)
```

